
********************************************************************************************************************
V1 trianing.py

import subprocess
# Run the installation script to get all packages
subprocess.run(['python', 'installs.py'])

# All should be installed after running the installation
import numpy as np
import random
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

try:
    from textProcessing import wordBag, tokenize, stemming
except ImportError:
    print(f"*****textProcessing.py could not be found please place the file in the project directory*****")
    print(f"    -Training program has stopped running due to error")
    raise SystemExit(1)

try:
    from model import network
except ImportError:
    print(f"*****model.py could not be found please place the file in the project directory*****")
    print(f"   -Training program has stopped running due to error")
    raise SystemExit(1)


print (f"Training program is running")
# Use the intents file
try:
    with open('intents.json', 'r') as f:
        intents = json.load(f)
except FileNotFoundError:
    print(f"*****intents.json could not be found, please place the file in the project directory*****")
    print(f"    -Training program has stopped running due to error")
    raise SystemExit(1)

# Create blank arrays to store tags, words and XY layers
allWords = []
tags = []
xy = []

# Loop through each portion of data
for intent in intents['intents']:
    tag = intent['tag']
    tags.append(tag)
    for pattern in intent['patterns']:
        sentenceWord = tokenize(pattern)
        allWords.extend(sentenceWord)
        xy.append((sentenceWord, tag))

ignoredLetters = ['?', '.', '!', '+', '-']
allWords = [stemming(sentenceWord) for sentenceWord in allWords if sentenceWord not in ignoredLetters]
allWords = sorted(set(allWords))
tags = sorted(set(tags))

x_train = []
y_train = []

for (sentencePattern, tag) in xy:
    bag = wordBag(sentencePattern, allWords)
    x_train.append(bag)
    label = tag.index(tag)
    y_train.append(label)

x_train = np.array(x_train)
y_train = np.array(y_train)
y_train = torch.LongTensor(y_train)

class chatbotDataser(Dataset):
    def __init__(self):
        self.n_samples = len(x_train)
        self.x_data = torch.Tensor(x_train)
        self.y_data = y_train

    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    def __len__(self):
        return self.n_samples


hiddenSize = 8
outputSize = len(tags)
inputSize = len(x_train[0])

batchSize = 8
learningRate = 0.001
numberEpochs = 1000


dataset = chatbotDataser()
trainLoader = DataLoader(dataset=dataset,
                          batch_size=batchSize,
                          shuffle=True,
                         num_workers=0
                         )
#gpu support
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

model = network(inputSize, hiddenSize, outputSize).to(device)


criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)

for epoch in range(numberEpochs):
    for (words, labels) in trainLoader:
        words = words.to(device)
        labels = labels.to(device)
        # Forward pass
        outputs = model(words)
        loss = criterion(outputs, labels)
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if (epoch + 1) % 100 == 0:
            print(f"Epoch [{epoch + 1}/{numberEpochs}], Loss: {loss.item():.4f}")

print(f'final loss, loss: {loss.item():.4f}')

data = {
"model_state": model.state_dict(),
    "input_size": inputSize,
    "output_size": outputSize,
    "hidden_size": hiddenSize,
    "all_words": allWords,
    "tags": tags,
}

try:
    FILE = "data.pth"
    torch.save(data, FILE)
    print(f"training complete, saved to {FILE}")
except FileNotFoundError:
    print(f"coule not save training data")
    print(f"Error: The file or directory specified by 'FILE' does not exist.")
except IOError:
    print(f"coule not save training data")
    print(f"Error: An I/O error occurred while saving the file. try replacing it with another data file.")

********************************************************************************************************************